#!/bin/bash
#This script will take 1 argument if needed [timeout] between google searches
#to potentially avoid rate limiting
timeout=$1
total_pages=0
count=0
domain=""
echo "What domain would you like to expand?"
read domain
echo "THIS SCRIPT WILL COMPOUND ON ANY PREVIOUS $domain-URLs.txt FILE"
echo "TO MAXIMIZE THE POOL OF SUBDOMAIN RESULTS THAT COULD BE FOUND"
echo "Enter the starting page (0 for 1st page)"
read count
count=$((count*10))
echo "How many search result pages?"
echo "(last search page; 10 results per page)"
read total_pages
total_pages=$((total_pages*10))
while [ $count -lt $total_pages ]; do
        echo "https://www.google.com/search?tbs=li:1&q=allinurl:+-www+site:$domain&start=$count"
        curl -A "Mozilla/5.0" -skLm 10 "https://www.google.com/search?tbs=li:1&q=allinurl:+-www+site:$domain&start=$count" | grep -oP '\/url\?q=.+?&amp' | sed 's|/url?q=||; s|&amp||' >> $domain-URLs.txt
sed -i '/webcache.googleusercontent.com/d' $domain-URLs.txt
        if [[ $timeout != "" ]]; then
        sleep $timeout
        fi
count=$((count+10))
done
cat $domain-URLs.txt | cut -d/ -f3 | sort | uniq > $domain-subdomains.txt
echo
cat $domain-subdomains.txt
echo
echo "All URLs found are in $domain-URLs.txt"
echo "All unique (sub)domains are in $domain-subdomains.txt"
echo "have a nice day"
